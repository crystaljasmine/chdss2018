---
title: '<div class="jumbotron"><h1 class="title toc-ignore display-3">Linear models in R</h1></div>'
author: "Danielle Navarro"
date: "5 December 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!--

  html_document:
    includes:
      in_header: header.html
    theme: flatly
    highlight: textmate
    css: mystyle.css

-->

```{r}
library(here)
library(tidyverse)
frames <- read_csv(here("analysis","data","frames_ex2.csv"))
```

## What this section is not

Because R is a *statistical* programming language it comes with a lot of hypothesis tests and tools built in, and of course there is an overwhelming number of packages out there that extend this. It is impossible to cover the whole thing in a brief tutorial, so I'm going to be a little picky. For example, I'm going to skip over the most commonly used classical tests, because they're comparatively easy to learn and it's not the best use of our time! For future reference though:

- The `t.test()` function handles one-sample, independent samples and paired samples t-tests
- The `chisq.test()` function handles chi-square tests of independence and Pearson goodness of fit tests
- The `prop.test()` function tests for the equality of two proportions.
- The `binom.test()` function allows you to do a binomial test of choice proportion against a known rate
- The `wilcox.test()` function handles one- and two-sample nonparametric tests of equality of means
- The `cor.test()` function tests the significance of a correlation

Of course, there are many, many others! What we're going to focus on here is:

- Linear modelling with the `lm()` function
- Linear mixed models with the `lmer()` function (in the `lme4` package)
- Generalised linear mixed models with the `glmer()` function (also from `lme4`)

## Linear models

Linear models should be fairly familiar to most: it's essentially what we were all taught in undergraduate under the name multiple regression. However, what is sometimes underemphasised is the fact that correlation, ANOVA, and t-tests can all be cast within the linear modelling framework, and R allows you do do all these using the `lm()` function. So that's where we're going to start. 

To begin with, we need a data set. For this purpose, let's construct a simplified version of the `frames` data, by averaging all the responses made by a person, regardless of the number of observations or the test item: 

```{r}
tinyframes <- frames %>%
  group_by(id, age, condition) %>%
  summarise(
    response = mean(response)
    ) %>%
  ungroup()
```

Let's take a look at the `tinyframes` dataset we've just created:

```{r}
glimpse(tinyframes)
```

A very typical way to produce descriptive statistics is to calculate mean and standard deviation for each condition, and count the number of people in each condition.

```{r}
tinyframes %>%
  group_by(condition) %>%
  summarise(
    mean_resp = mean(response), 
    sd_resp = sd(response),
    n = n()
  )
```

We would also want to visualise the data. It is almost always a mistake to start trying to model a data set without properly exploring it and making sure you have a good "feel" for what is going on. So let's draw a picture!

```{r}
tinyframes %>% 
  ggplot(aes(x = age, y = response, colour = condition)) + 
  geom_smooth(method = "lm") + 
  geom_point()
```

Intuitively, it looks like the two conditions are likely to be different to one another; but if there's any effect of age it would have to be tiny. 

### Using lm to do a t-test

So let's start with a simple question. Is there a "significant" difference between the two conditions? I'm not a fan of orthodox null hypothesis testing, to be honest, but it does have it's place. Traditionally, the solution is the t-test:

```{r}
t.test(
  formula = response ~ condition, 
  data = tinyframes, 
  var.equal = TRUE
)
```

Okay we have a significant difference. So we reject the null hypothesis (i.e., that the two groups have the same population mean) and accept the alternative (i.e., that they have different population means). Yay. I guess. The moment we start caring about data analysis in any detail, though, it helps to recast these "hypotheses" in terms of *statistical models*. 

```{r}
mod1 <- lm(formula = response ~ 1, data = tinyframes) 
mod2 <- lm(formula = response ~ condition, data = tinyframes)
```

To give you a sense of what R has just done, it has estimated the coefficients for two different regression models: `mod1` only includes an intercept term (i.e., the "grand mean"), wherese `mod2` contains two terms:

```{r}
mod2
```

Notice that the coefficients have a clear relationship to the group means: the "intercept" term is identical to the group mean for category sampling, and the "conditionproperty" term is what you have to add to that to get the group mean for property sampling (i.e., 5.4 - 1.0 = 4.4). It's expressed in different language than the t-test, but `mod2` nevertheless maps onto the alternative hypothesis. 

To compare these two linear models, we can call the `anova()` function:

```{r}
anova(mod1, mod2)
```

This doesn't look like the output of a t-test, but if we look carefully we notice that:

- The $p$-values are the same
- The test statistics are related: $t = \sqrt{F}$
- The residual df in the ANOVA table is the same as the t-test df

In a sense, what we've just done is illustrate the fact that the Student t-test is equivalent to a one-way ANOVA with two groups (which we were all taught as undergraduates), but we've used linear models to do it! 

Before moving on, I should mention that this is a slightly different ANOVA table than what you might be expecting to see. You can get the more traditional version like this,

```{r}
anova(lm(response ~ condition, tinyframes))
```

but it's essentially the same thing and I don't want to spend too much time on this since the focus is really on how we can extend these ideas. 

### Mixing continuous and discrete predictors

One nice thing about linear models is that, because the framework is pretty general, there's nothing stopping you from including continuous variables and categorical variables in the same model (as one might do in an ANCOVA). So, sticking with out `tinyframes` data, perhaps we should check to see if we can detect an effect of age. It doesn't look very likely, but let's run the model anyway. What we'll do this time is take the same two models as before `mod1` and `mod2`, but now add a third model that includes `age` as an additional predictor:

```{r}
mod3 <- lm(formula = response ~ condition + age, data = tinyframes)
```

If we take a quick look at the coefficients

```{r}
mod3
```

we can see that age really isn't having much of an effect, if any. To compare all three models using an $F$-test, what we would do is this:

```{r}
anova(mod1, mod2, mod3)
```

In this ANOVA table, what you're looking at is a test of `mod2` against `mod1`, followed by `mod3` against `mod2`. This suggests that `mod2` is preferred over `mod1` (reject the null), but `mod3` isn't preferred over `mod2` (retain the null).

### From hypothesis tests to model selection

I have a confession to make. I really dislike using null hypothesis tests the way we did in the previous analysis. Most of the framework for null hypothesis testing comes from work by Jerzy Neyman, and in his framework the goal is to *prespecify* a formal procedure such that you can input a data set, and output a binary decision; and specifically to ensure that this decision procedure controls your Type 1 error rate. This approach to statistics has its place, but it's not actually all that consistent with what we're doing here. Neyman's approach is completely automated: you *must* follow the procedure no matter what your data looks like, or else your Type 1 error isn't controlled. So if explore your data and they turn out to be super weird, you must apply your prespecified decision rule. If you don't not only is the p-value for this test completely meaningless, it also strongly invalidates any other p-values you report, even if you did stick to the procedure in those cases: because it implies that, had the data turned out some other way, you wouldn't have stuck to the procedure, and Neyman's theory only works if you *always* follow the prespecified analysis plan. No excuses, no exceptions. 

In real life, this never actually happens. Science doesn't work that way. What I find bizarre, is that while statisticians for the most part have recognised that this presents a problem for Neyman's theory, there is a school of thought within psychology that the problem lies with the *scientist* for not adhering to this stupid statistical theory. Well, nuts to that. I have no particular interest in following Neyman's absurd rules, and my main goal as a scientist is something more akin to Ockham's razor: find the simplest model that provides an good enough account of the data.

In essence, what I've done here is reframed the statistical problem, and changed it from a "hypothesis testing" problem to a "model selection" problem. The tools used for model selection are often somewhat different, and the underlying philosophy is often more aligned with the Ockham's razor idea. Two (very simple, and often flawed) approaches to this are the Akaike information criterion (AIC) and the Bayesian information criterion (BIC), both of which have been around since the 1970s. For our linear models, we can evaluate them using the `AIC()` and `BIC()` functions: 

```{r}
AIC(mod1, mod2, mod3)
BIC(mod1, mod2, mod3)
```

Smaller values of AIC and BIC are better, and it's hardly a surprise that `mod2` turns out to be the best one!

### Exploring the model

Overall, `mod2` looks pretty sensible:

```{r}
summary(mod2)
confint(mod2)
```

TODO:

- the `predict()` function
- the `residuals()` function
- regression diagnostics using `plot()`

